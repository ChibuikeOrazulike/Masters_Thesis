{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c27647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data frame\n",
    "main_df = pd.read_excel('Selected_Springs_final_updated.xlsx')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0477e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculated the distance between each spring coordinate and the closest grid point\n",
    "\n",
    "storage_df = pd.read_csv('GWS.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Defined the Haversine distance function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Found the closest storage latitude-longitude pairs for each spring\n",
    "def closest_storage(spring_row):\n",
    "    spring_lat = spring_row['Latitude']\n",
    "    spring_lon = spring_row['Longitude']\n",
    "    distances = storage_df.apply(lambda row: haversine(spring_lat, spring_lon, row['lat'], row['lon']), axis=1)\n",
    "    closest_storage_index = distances.idxmin()\n",
    "    closest_row = storage_df.iloc[closest_storage_index]\n",
    "    distance = distances[closest_storage_index]\n",
    "    return pd.Series([closest_row['lon'], closest_row['lat'], distance])\n",
    "\n",
    "# Applied the function to the spring data frame and create the output data frame\n",
    "output_columns = ['longitude', 'latitude', 'distance'] \n",
    "output_df = main_df.apply(closest_storage, axis=1, result_type='expand')\n",
    "output_df.columns = output_columns\n",
    "\n",
    "\n",
    "# Saved the output data frame\n",
    "#output_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bee589",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_df['Start date'] = main_df['Start date']\n",
    "output_df['End date'] = main_df['End date']\n",
    "output_df['Name'] = main_df['Name']\n",
    "output_df['WoKAS ID'] = main_df['WoKaS_ID']\n",
    "output_df.to_csv('Spring New Coordinates.csv', index=None)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_df = pd.read_csv('GWS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Step 1: Extracted latitude and longitude values from output_df\n",
    "latitude_list = output_df['latitude'].tolist()\n",
    "longitude_list = output_df['longitude'].tolist()\n",
    "\n",
    "# Step 2: Converted \"Start date\" and \"End date\" columns to datetime objects\n",
    "output_df['Start date'] = pd.to_datetime(output_df['Start date'])\n",
    "output_df['End date'] = pd.to_datetime(output_df['End date'])\n",
    "\n",
    "# Step 3: Converted the time series columns of storage_df to datetime objects\n",
    "time_columns = storage_df.columns[2:]  # Excluding 'lat' and 'lon' columns\n",
    "\n",
    "# Step 4: Filtered storage_df based on latitude, longitude, and time period\n",
    "subset_list = []\n",
    "\n",
    "for lat, lon, start_date, end_date in zip(latitude_list, longitude_list, output_df['Start date'], output_df['End date']):\n",
    "    # Updated the start_date to the first day of the month if not already\n",
    "    if start_date.day != 1:\n",
    "        start_date = start_date.replace(day=1)\n",
    "\n",
    "    # If end_date is the first day of the month, update it to the second day\n",
    "    if end_date.day == 1:\n",
    "        end_date = end_date.replace(day=2)\n",
    "        \n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    filtered_df = storage_df.loc[(storage_df['lat'] == lat) & (storage_df['lon'] == lon), time_columns]\n",
    "    \n",
    "    # Filtered columns based on start_date_str and end_date_str\n",
    "    subset = filtered_df.loc[:, start_date_str:end_date_str]\n",
    "    subset_list.append(subset)\n",
    "\n",
    "# Step 5: Retrieved the subset of storage_df with desired columns\n",
    "result_df = pd.concat(subset_list, axis=0)\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "result_df\n",
    "\n",
    "# result_df should now contain the desired subset of storage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec55fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#THIS PARTICULAR CODE CALCULATES THE MONTHLY MEAN OF DISCHARGE AND USES LINEAR INTERPOLATION \n",
    "#FOR THE VALUES IN THE DATAFRAMES WITH SOME MISSING MONTHS\n",
    "#I THINK THIS IS MORE FAVORABLE\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "for index, row in main_df.iterrows():\n",
    "    input_file_name = row['WoKaS_ID'] + '.csv'\n",
    "    input_file_path = os.path.join(folder_path, input_file_name)\n",
    "\n",
    "    output_file_name = 'converted_' + input_file_name\n",
    "\n",
    "    df = pd.read_csv(input_file_path, skiprows=8, encoding='latin1')\n",
    "    df[['Timestamp', 'discharge']] = df['Timestamp\\tdischarge'].str.split('\\t', expand=True)\n",
    "    df.drop(columns=['Timestamp\\tdischarge'], inplace=True)\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d.%m.%Y %H:%M:%S')\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    start_date = row['Start date']\n",
    "    end_date = row['End date']\n",
    "\n",
    "    filtered_df = df.loc[start_date:end_date]\n",
    "\n",
    "    filtered_df['discharge'] = pd.to_numeric(filtered_df['discharge'], errors='coerce')\n",
    "    monthly_discharge_df = filtered_df.resample('M').mean()\n",
    "\n",
    "    # Performed linear interpolation for missing discharge values\n",
    "    monthly_discharge_df['discharge'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "    monthly_discharge_df.reset_index(inplace=True)\n",
    "\n",
    "    # Retrieved the corresponding row from result_df\n",
    "    storage_row = result_df.iloc[index]\n",
    "\n",
    "    # Droped columns containing NaN values in storage_row\n",
    "    storage_row = storage_row.dropna()\n",
    "\n",
    "    # Added 'storage' column to monthly_discharge_df and fill with values from storage_row\n",
    "    monthly_discharge_df = monthly_discharge_df.assign(storage=storage_row.values)\n",
    "\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    monthly_discharge_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30dadb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c629eee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_path = \"./\"\n",
    "files = glob.glob(os.path.join(folder_path, 'converted_*.csv'))\n",
    "\n",
    "results = []\n",
    "\n",
    "for file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    spearman_corr, p_value = spearmanr(df['storage'], df['discharge'])\n",
    "    \n",
    "    results.append({\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "Spearman_df = pd.DataFrame(results)\n",
    "Spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶Lagging the Discharge by one Month◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bdaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Specifed the folder path and the file pattern\n",
    "folder_path = './'\n",
    "file_pattern = 'converted_*.csv'  # Adjust the file extension if needed\n",
    "\n",
    "# Got the list of all matching files in the folder\n",
    "files = glob.glob(f\"{folder_path}/{file_pattern}\")\n",
    "\n",
    "# Looped through each file and apply the processing steps\n",
    "for file in files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Applied the processing steps as before\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    df['lagged_discharge'] = df['discharge'].shift(1)#SHIFTED BY ONE MONTH\n",
    "    avg_lagged_discharge = df['lagged_discharge'].mean()\n",
    "    df['lagged_discharge'].fillna(avg_lagged_discharge, inplace=True) #THIS REPLACES THE INITIAL DISPLACED VAULE WITH THE MEAN\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Timestamp'] = df['Timestamp'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "  # Saved the processed DataFrame to a new CSV file with the prefix \"lagged\" added to the current name\n",
    "    output_file_name = 'lagged_' + file.split('\\\\')[-1]\n",
    "    output_file = f\"{folder_path}/{output_file_name}\"\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96977fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "folder_path = \"./\"\n",
    "files = glob.glob(os.path.join(folder_path, 'lagged_*.csv'))\n",
    "\n",
    "results = []\n",
    "\n",
    "for file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    spearman_corr, p_value = spearmanr(df['storage'], df['lagged_discharge'])\n",
    "    \n",
    "    results.append({\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "Spearman_df_lagged = pd.DataFrame(results)\n",
    "Spearman_df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶Plotting 1◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spearman_df['latitude'] = output_df['latitude']\n",
    "Spearman_df['longitude'] = output_df['longitude']\n",
    "Spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39059224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61452e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶Plotting◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25704950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('converted_FR-0050.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Timestamp = pd.to_datetime(df.Timestamp)\n",
    "fig, ax1 = plt.subplots(figsize=(15, 7))  # Adjusted the width and height of the graph\n",
    "\n",
    "# Plotted discharge in red on the left y-axis\n",
    "ax1.plot(df['Timestamp'], df['discharge'], color='red', label='Discharge')\n",
    "ax1.tick_params(axis='x', labelsize=15)\n",
    "ax1.set_xlabel('Timestamp', fontsize=20)\n",
    "ax1.set_ylabel('Discharge', color='red', fontsize=20)\n",
    "ax1.tick_params(axis='y', labelcolor='red', labelsize=20)\n",
    "\n",
    "# Created a second y-axis for storage\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plotted storage in blue on the right y-axis\n",
    "ax2.plot(df['Timestamp'], df['storage'], color='blue', label='Storage')\n",
    "ax2.set_ylabel('Storage', color='blue',fontsize=20)\n",
    "ax2.tick_params(axis='y', labelcolor='blue', labelsize=20 )\n",
    "\n",
    "# Set the title and show the plot\n",
    "plt.title('Spring Discharge (FR-0057) and GWS (without soil moisture)', fontsize = 22)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1233ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('lagged_converted_FR-0050.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68830179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Timestamp = pd.to_datetime(df.Timestamp)\n",
    "fig, ax1 = plt.subplots(figsize=(15, 7))  # Adjusted the width and height of the graph\n",
    "\n",
    "# Plotted discharge in red on the left y-axis\n",
    "ax1.plot(df['Timestamp'], df2['lagged_discharge'], color='red', label='Discharge')\n",
    "ax1.tick_params(axis='x', labelsize=20)\n",
    "ax1.set_xlabel('Timestamp', fontsize=20)\n",
    "ax1.set_ylabel('Discharge', color='red', fontsize=20)\n",
    "ax1.tick_params(axis='y', labelcolor='red', labelsize=18)\n",
    "\n",
    "# Created a second y-axis for storage\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plotted storage in blue on the right y-axis\n",
    "ax2.plot(df['Timestamp'], df2['storage'], color='blue', label='Storage')\n",
    "ax2.set_ylabel('Storage', color='blue',fontsize=20)\n",
    "ax2.tick_params(axis='y', labelcolor='blue', labelsize=18 )\n",
    "\n",
    "# Set the title and show the plot\n",
    "plt.title('Lagged Spring Discharge (FR-0057) and GWS (without soil moisture)', fontsize = 22)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392dada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶▶CLUSTER ANALYSIS OF THE SPRING◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀◀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613784dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported a dataframe containing the springs, their catchment sizes and correlation coefficient\n",
    "\n",
    "df = pd.read_csv('cluster.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dbda6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Assigned a number for each spring class from C1 to C6, this will help in visualization\n",
    "class_mapping = {'C1': 1, 'C2': 2, 'C3': 3, 'C4': 4, 'C5': 5, 'C6': 6}\n",
    "df['class_number'] = df['spring_class'].map(class_mapping)\n",
    "# Created a cluster plot using seaborn library\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='class_number', y='correlation_coefficient', hue='spring_class', palette='deep', s=200) # Increase the value of s to increase point size\n",
    "\n",
    "plt.xlabel('Spring Class', fontsize=20)\n",
    "plt.ylabel('Correlation Coefficient', fontsize=20)\n",
    "plt.title('Cluster Plot of Correlation of Classes of Spring', fontsize=20)\n",
    "\n",
    "# Customized the x-axis with the class labels\n",
    "plt.xticks(range(1, 7), ['C1', 'C2', 'C3', 'C4', 'C5', 'C6'])\n",
    "\n",
    "# Added catchment_size labels for each point\n",
    "for idx, row in df.iterrows():\n",
    "    plt.text(row['class_number'] + 0.05, row['correlation_coefficient'], row['catchment_size'], fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90401440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cluster2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ef791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Created a scatter plot using Matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(df['correlation_coefficient'], df['catchment_size'])\n",
    "\n",
    "# Calculated the trendline\n",
    "z = np.polyfit(df['correlation_coefficient'], df['catchment_size'], 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Plotted the trendline in red\n",
    "plt.plot(df['correlation_coefficient'], p(df['correlation_coefficient']), 'r-')\n",
    "\n",
    "plt.xlabel('Correlation Coefficient', fontsize= 20)\n",
    "plt.ylabel('Catchment Size (km²)', fontsize= 20)\n",
    "plt.title('Scatter Plot of Catchment Size vs Correlation Coefficient with Trendline', fontsize= 20)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949599a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot of Catchment Size vs Correlation Coefficient with Trendline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Created a scatter plot using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['correlation_coefficient'], df['catchment_size'])\n",
    "\n",
    "# Calculated the trendline\n",
    "z = np.polyfit(df['correlation_coefficient'], df['catchment_size'], 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Plotted the trendline in red\n",
    "plt.plot(df['correlation_coefficient'], p(df['correlation_coefficient']), 'r-')\n",
    "\n",
    "# Added text labels for each point\n",
    "for i, txt in enumerate(df['WoKAS ID']):\n",
    "    plt.annotate(txt, (df['correlation_coefficient'][i], df['catchment_size'][i]))\n",
    "\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Catchment Size')\n",
    "plt.title('Scatter Plot of Catchment Size vs Correlation Coefficient with Trendline')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
